@article{Chen2012,
abstract = {We extend the herding algorithm to continuous spaces by using the kernel trick. The resulting "kernel herding" algorithm is an infinite memory deterministic process that learns to approximate a PDF with a collection of samples. We show that kernel herding decreases the error of expectations of functions in the Hilbert space at a rate O(1/T) which is much faster than the usual O(1/pT) for iid random samples. We illustrate kernel herding by approximating Bayesian predictive distributions.},
archivePrefix = {arXiv},
arxivId = {1203.3472},
author = {Chen, Yutian and Welling, Max and Smola, Alex},
eprint = {1203.3472},
file = {:home/gosha/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Welling, Smola - 2012 - Super-Samples from Kernel Herding.pdf:pdf},
month = {mar},
title = {{Super-Samples from Kernel Herding}},
url = {http://arxiv.org/abs/1203.3472},
year = {2012}
}

@article{Chen2010,
abstract = {A parametric version of herding is formulated. The nonlinear mapping between consecutive time slices is learned by a form of self-supervised training. The resulting dynamical system generates pseudo-samples that resemble the original data. We show how this parametric herding can be successfully used to compress a dataset consisting of binary digits. It is also verified that high compression rates translate into good prediction performance on unseen test data.},
author = {Chen, Yutian and Welling, Max},
file = {:home/gosha/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Welling - Unknown - Parametric Herding.pdf:pdf},
mendeley-groups = {Аспирантура/курсы/баес осень 2020},
title = {{Parametric Herding}},
year = {2010}
}

@article{Welling2,
abstract = {A new "herding" algorithm is proposed which directly converts observed moments into a sequence of pseudo-samples. The pseudo-samples respect the moment constraints and may be used to estimate (unobserved) quantities of interest. The procedure allows us to sidestep the usual approach of first learning a joint model (which is intractable) and then sampling from that model (which can easily get stuck in a local mode). Moreover, the algorithm is fully deterministic, avoiding random number generation) and does not need expensive operations such as exponentiation.},
author = {Welling, Max and Bren, Donald},
file = {:home/gosha/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Welling, Bren - Unknown - Herding Dynamical Weights to Learn.pdf:pdf},
mendeley-groups = {Аспирантура/курсы/баес осень 2020},
title = {{Herding Dynamical Weights to Learn}}
}

@article{Welling3,
abstract = {Learning the parameters of a (potentially partially observable) random field model is intractable in general. Instead of focussing on a single optimal parameter value we propose to treat parameters as dynamical quantities. We introduce an algorithm to generate complex dynamics for parameters and (both visible and hidden) state vectors. We show that under certain conditions averages computed over trajectories of the proposed dynamical system converge to averages computed over the data. Our "herding dynamics" does not require expensive operations such as exponentiation and is fully deterministic.},
author = {Welling, Max},
file = {:home/gosha/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Welling - Unknown - Herding Dynamic Weights for Partially Observed Random Field Models.pdf:pdf},
mendeley-groups = {Аспирантура/курсы/баес осень 2020},
title = {{Herding Dynamic Weights for Partially Observed Random Field Models}}
}

@article{Welling2010,
abstract = {We describe a class of deterministic weakly chaotic dynamical systems with infinite memory. These "herding systems" combine learning and inference into one algorithm, where moments or data-items are converted directly into an arbitrarily long sequence of pseudo-samples. This sequence has infinite range correlations and as such is highly structured. We show that its information content, as measured by sub-extensive entropy, can grow as fast as K log T , which is faster than the usual 1 2 K log T for exchangeable sequences generated by random posterior sampling from a Bayesian model. In one dimension we prove that herding sequences are equivalent to Sturmian sequences which have complexity exactly log(T + 1). More generally, we advocate the application of the rich theoretical framework around nonlinear dynamical systems, chaos theory and fractal geometry to statistical learning.},
author = {Welling, Max and Chen, Yutian},
doi = {10.1088/1742-6596/233/1/012005},
file = {:home/gosha/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaves et al. - 2010 - Conference Series OPEN ACCESS To cite this article Max Welling and Yutian Chen.pdf:pdf},
journal = {J. Phys.: Conf. Ser},
mendeley-groups = {Аспирантура/курсы/баес осень 2020},
pages = {12005},
title = {{Conference Series OPEN ACCESS To cite this article: Max Welling and Yutian Chen}},
volume = {233},
year = {2010}
}
